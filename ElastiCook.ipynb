{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you run for first time uncomment these\n",
    "# !pip install elasticsearch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from scipy import spatial\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "no_of_recepies = 500 # Total = 124473\n",
    "ingredient_list = 'Filterd_ingredient' # ingredients (otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Pickel File for Filtered Data and creating dictionary of recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multiple files found in ZIP file. Only one file per ZIP: ['Cleaned_recipes.P', '__MACOSX/._Cleaned_recipes.P']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# 1) try standard libary Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    380\u001b[0m                     raise ValueError(\n\u001b[1;32m    381\u001b[0m                         \u001b[0;34m\"Multiple files found in ZIP file.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                         \u001b[0;34m\" Only one file per ZIP: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                     )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Multiple files found in ZIP file. Only one file per ZIP: ['Cleaned_recipes.P', '__MACOSX/._Cleaned_recipes.P']"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gagan's Code\n",
    "import pandas as pd\n",
    "rec_df = pd.read_pickle(\"Cleaned_recipes.P.zip\")\n",
    "rec_l = []\n",
    "temp_dict = rec_df.to_dict(orient='index')\n",
    "for ind in temp_dict:\n",
    "    rec_l.append(temp_dict[ind])\n",
    "es = Elasticsearch()\n",
    "for i in range(0,len(rec_l[:no_of_recepies])): #i took only first 120 recipes because time and size\n",
    "    res = es.index(index='recipes', doc_type='recipe', id=i, body=rec_l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# recipes = []\n",
    "# with open('recipes_raw_nosource_ar.json') as json_file: #make sure the recipe json is in same dir\n",
    "#     data = json.load(json_file)\n",
    "#     for p in data:\n",
    "#         recipes.append(data[p])\n",
    "# es = Elasticsearch()\n",
    "# for i in range(0,len(recipes[:500])): #i took only first 120 recipes because time and size\n",
    "#     res = es.index(index='recipes', doc_type='recipe', id=i, body=recipes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveResults(query, last, wish,title=True,instructions=True):\n",
    "    res = es.search(index=\"recipes\", body={\"query\": {\"bool\": { \"must\": [{\"match\":{ingredient_list:query} }]}},\"size\":no_of_recepies})\n",
    "    mscores = [] # new scores for personalized results\n",
    "    m2scores = [] #new scores for advanced personalized results\n",
    "    results = [] #unpersonalized\n",
    "    presults = [] #personalized \n",
    "    p2results = [] #adv personalized\n",
    "    for hit in res['hits']['hits']:\n",
    "        if title:\n",
    "            results.append((hit['_source']['title'],hit['_score']))\n",
    "        if instructions: \n",
    "            results.append((hit['_source']['instructions'],hit['_score']))\n",
    "        ingr = ''\n",
    "        for s in hit['_source'][ingredient_list]:\n",
    "            ingr += s\n",
    "        ldocuments = (ingr,last)\n",
    "        wdocuments = (ingr,wish)\n",
    "        tfidf_vectorizer=TfidfVectorizer()\n",
    "        tfidf_last=tfidf_vectorizer.fit_transform(ldocuments)\n",
    "        tfidf_wish=tfidf_vectorizer.fit_transform(wdocuments)\n",
    "        cs = -100 * cosine_similarity(tfidf_last[0:1],tfidf_last)\n",
    "        ds = 100 * cosine_similarity(tfidf_wish[0:1],tfidf_wish)\n",
    "        mscores.append((hit['_source'],hit['_score']+cs[0][1]))\n",
    "        m2scores.append((hit['_source'],hit['_score']+cs[0][1]+ds[0][1]))\n",
    "    mscores.sort(key=lambda t: t[1],reverse=True)\n",
    "    m2scores.sort(key=lambda t: t[1],reverse=True)\n",
    "    for t in mscores:\n",
    "        if title:\n",
    "            presults.append((t[0]['title'],t[1]))\n",
    "        if instructions:\n",
    "            presults.append((t[0]['instructions'],t[1]))\n",
    "    for t in m2scores:\n",
    "        if title:\n",
    "            p2results.append((t[0]['title'],t[1]))\n",
    "        if instructions:\n",
    "            p2results.append((t[0]['instructions'],t[1]))\n",
    "    return presults,p2results, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput():\n",
    "    query = input(\"Enter whatever ingredients you have in your pantry seperated by whitespaces: \")\n",
    "    last = input(\"Enter at least 5 ingredients you ate last week seperated by whitespace: \")\n",
    "    wish = input(\"List anything you would like to eat. You can also just skip this.\")\n",
    "    return query,last,wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveRecipes(title = True, instructions = False):\n",
    "    query,last,wish = getInput()\n",
    "    print(query)\n",
    "    personalized, personalized2, standard = giveResults(query,last,wish,title,instructions)\n",
    "    return personalized, personalized2, standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter whatever ingredients you have in your pantry seperated by whitespaces: tofu tuna beef\n",
      "Enter at least 5 ingredients you ate last week seperated by whitespace: fish cheese rice\n",
      "List anything you would like to eat. You can also just skip this.meat\n",
      "tofu tuna beef\n"
     ]
    }
   ],
   "source": [
    "p1,p2,s = giveRecipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Easy Tuna Casserole', 5.712923),\n",
       " ('Best Tuna Casserole', 4.882046),\n",
       " (\"Barbie's Tuna Salad\", 4.450528),\n",
       " ('Tuna Noodle Casserole from Scratch', 4.0890985),\n",
       " ('Cabbage Roll Casserole', 3.4308121),\n",
       " ('High Temperature Eye-of-Round Roast', 3.1109436),\n",
       " (\"Shepherd's Pie VI\", 2.9607046),\n",
       " ('BBQ Pork for Sandwiches', 2.9238453),\n",
       " ('Bacon Wrapped Smokies', 2.9238453),\n",
       " ('Slow Cooker Beef Stew I', 2.873215)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:10] #standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Easy Tuna Casserole', 5.712923),\n",
       " ('Best Tuna Casserole', 4.882046),\n",
       " (\"Barbie's Tuna Salad\", 4.450528),\n",
       " ('Cabbage Roll Casserole', 3.4308121),\n",
       " ('High Temperature Eye-of-Round Roast', 3.1109436),\n",
       " ('BBQ Pork for Sandwiches', 2.9238453),\n",
       " ('Bacon Wrapped Smokies', 2.9238453),\n",
       " ('Slow Cooker Beef Stew I', 2.873215),\n",
       " ('Foolproof Rib Roast', 2.8384893),\n",
       " ('Hamburger Steak with Onions and Gravy', 2.8313808)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[:10] #personalized using only last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Slow Cooker Beef Stew I', 22.233539687858617),\n",
       " ('Slow Cooker Beef Stroganoff I', 16.907940277983457),\n",
       " ('Easy Tuna Casserole', 5.712923),\n",
       " ('Best Tuna Casserole', 4.882046),\n",
       " (\"Barbie's Tuna Salad\", 4.450528),\n",
       " ('Cabbage Roll Casserole', 3.4308121),\n",
       " ('High Temperature Eye-of-Round Roast', 3.1109436),\n",
       " ('BBQ Pork for Sandwiches', 2.9238453),\n",
       " ('Bacon Wrapped Smokies', 2.9238453),\n",
       " ('Foolproof Rib Roast', 2.8384893)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[:10]#personalized using last week and wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
